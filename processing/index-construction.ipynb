{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: pymongo in /home/tobias/anaconda3/lib/python3.7/site-packages (3.10.1)\nRequirement already satisfied: pycld2 in /home/tobias/anaconda3/lib/python3.7/site-packages (0.41)\n"
    }
   ],
   "source": [
    "!pip install pymongo\n",
    "!pip install pycld2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import pycld2 as cld2\n",
    "\n",
    "from multiprocessing.pool import Pool\n",
    "from tqdm._tqdm_notebook import tqdm\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from os import getcwd, pardir\n",
    "from os.path import join, basename\n",
    "\n",
    "from utils.literature import DataLoader\n",
    "from utils.preprocessing import NLPPipeline, Tokenizer, Stemmer, ToLowercase, Lemmatizer, StopwordRemover, CitationRemover, SymbolRemover, ContentInBracketsRemover\n",
    "from glob import glob\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer #TODO: Try lemmatizer instead of stemmer\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient as DBClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification of the document paths\n",
    "\n",
    "- download from: [Kaggle](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)\n",
    "- extract folder content to `{ProjectDir}/dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = join(pardir, 'dataset')\n",
    "\n",
    "json_paths = [\n",
    "    join(root_dir, 'arxiv', 'arxiv', 'pdf_json'),\n",
    "    join(root_dir, 'arxiv', 'arxiv', 'pdf_json'),\n",
    "    join(root_dir, 'comm_use_subset', 'comm_use_subset', 'pdf_json'),\n",
    "    join(root_dir, 'noncomm_use_subset', 'noncomm_use_subset', 'pdf_json'),\n",
    "    join(root_dir, 'custom_license', 'custom_license', 'pdf_json'),\n",
    "    join(root_dir, 'biorxiv_medrxiv', 'biorxiv_medrxiv', 'pdf_json'),\n",
    "]\n",
    "\n",
    "files = []\n",
    "[files.extend(glob(join(path, '*.json'))) for path in json_paths];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the document index\n",
    "\n",
    "Each document gets stored in the document_index collection with the following layout: \n",
    "```json\n",
    "{\n",
    "    _id: ..., \n",
    "    document_title: ...\n",
    "}\n",
    "```\n",
    "\n",
    "## Building the inverted index\n",
    "\n",
    "```json\n",
    "{\n",
    "    'stem': ...,\n",
    "    'doc_ids': [\n",
    "        {\n",
    "            {'doc_id': ..., 'count': ...},\n",
    "            {'doc_id': ..., 'count': ...},\n",
    "            {'doc_id': ..., 'count': ...},\n",
    "            {'doc_id': ..., 'count': ...},\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DBClient('localhost', 27017, w=0) as client:\n",
    "    db = client['covid_19']\n",
    "    document_index_collection = db['document_index']\n",
    "    inverted_index_collection = db['inverted_index']\n",
    "\n",
    "    if '_id' not in document_index_collection.index_information():\n",
    "        document_index_collection.create_index('_id')\n",
    "\n",
    "    index_names = ['_id', 'doc_ids.doc_id']\n",
    "\n",
    "    for index_name in index_names:\n",
    "        if index_name not in document_index_collection.index_information():\n",
    "            inverted_index_collection.create_index(index_name, unique=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Test\n",
    "pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = NLPPipeline([\n",
    "    ToLowercase(),\n",
    "    CitationRemover(),\n",
    "    ContentInBracketsRemover(),\n",
    "    Tokenizer(),\n",
    "    SymbolRemover(),\n",
    "    StopwordRemover(),\n",
    "    Lemmatizer()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(stemmed_tokens, count = 20):\n",
    "    is_reliable, _, details = cld2.detect(' '.join(stemmed_tokens[:count]))\n",
    "    if not is_reliable or details[0][1] != 'en':\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_text(fpath, dl=None):\n",
    "    if dl is None: dl = DataLoader(fpath)\n",
    "    return dl.get_full_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_title(fpath, dl=None):\n",
    "    if dl is None: dl = DataLoader(fpath)\n",
    "    return dl.get_title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_post(stem, doc_id):\n",
    "    return {'_id': stem, 'doc_ids': [{'doc_id': doc_id, 'count': 1}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(args):\n",
    "    with DBClient('localhost', 27017, w=0) as client:\n",
    "        db = client['covid_19']\n",
    "        document_index_collection = db['document_index']\n",
    "        inverted_index_collection = db['inverted_index']\n",
    "restart0since Tue May 12 2020\n",
    "        for fpath, doc_id in args:\n",
    "            doc_id = int(doc_id)\n",
    "\n",
    "            #preprocessing\n",
    "            dl = DataLoader(fpath)\n",
    "            text = get_full_text(fpath, dl)\n",
    "            stemmed_tokens = pipeline.transform(text)\n",
    "\n",
    "            #get relevant metadata\n",
    "            document_title = get_document_title(fpath, dl)\n",
    "\n",
    "            if not is_english(stemmed_tokens) or document_title == '':\n",
    "                continue\n",
    "\n",
    "            #update document index\n",
    "            doc_id_entry = document_index_collection.find({'_id': doc_id}).limit(1)\n",
    "            if doc_id_entry.count() == 0:\n",
    "                document_index_collection.insert_one({'_id': doc_id, 'document_title': document_title})\n",
    "\n",
    "            #update inverted index\n",
    "            for stem in stemmed_tokens:\n",
    "                stem_entry = inverted_index_collection.find({'_id': stem}).limit(1)\n",
    "                if stem_entry.count() > 0:\n",
    "                    #try to find current document id in stem entry\n",
    "                    doc_id_object = inverted_index_collection.find(\n",
    "                        {'_id': stem, \n",
    "                        'doc_ids': {'$elemMatch': {'doc_id': doc_id}}}).limit(1)\n",
    "                    if doc_id_object.count() > 0:\n",
    "                        # update occurrence of stem in document\n",
    "                        inverted_index_collection.update(\n",
    "                            {'_id': stem, \n",
    "                            'doc_ids': {'$elemMatch': {'doc_id': doc_id}}},\n",
    "                            {'$inc': {'doc_ids.$.count': 1}})\n",
    "                    else:\n",
    "                        # add document id\n",
    "                        inverted_index_collection.update(\n",
    "                            {'_id': stem},\n",
    "                            {'$push': {'doc_ids': {'doc_id': doc_id, 'count': 1}}})\n",
    "                else:\n",
    "                    post = create_post(stem, doc_id)\n",
    "                    inverted_index_collection.insert_one(post)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(files, chunk_size=100):\n",
    "    doc_ids = list(range(len(files)))\n",
    "    chunks = list()\n",
    "    for i in range(0, len(files), chunk_size):\n",
    "        indices = np.array(doc_ids[i: min(i+chunk_size, len(files))])\n",
    "        chunks.append(list(zip(\n",
    "            files[i: min(i+chunk_size, len(files))], \n",
    "            doc_ids[i: min(i+chunk_size, len(files))])))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=492.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1bb404cc46fa42b89a67b0869a75e5e2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    }
   ],
   "source": [
    "pool = Pool()\n",
    "chunks = create_chunks(files)\n",
    "\n",
    "for _ in tqdm(pool.imap_unordered(process_chunk, chunks), total=len(chunks)):\n",
    "    pass\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbasecondaae23e32a32964a9396c6d021c6ebb9de"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}