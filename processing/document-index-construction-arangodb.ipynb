{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install python-arango\n",
    "!pip install pycld2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import pycld2 as cld2\n",
    "\n",
    "from multiprocessing.pool import Pool\n",
    "from tqdm._tqdm_notebook import tqdm\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from os import getcwd, pardir\n",
    "from os.path import join, basename\n",
    "\n",
    "from utils.literature import DataLoader, get_document_title, is_english, get_section, get_sections\n",
    "from utils.preprocessing import NLPPipeline, Tokenizer, Stemmer, ToLowercase, Lemmatizer, StopwordRemover, CitationRemover, SymbolRemover, ContentInBracketsRemover, NonAlphanumericRemover\n",
    "from glob import glob\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer #TODO: Try lemmatizer instead of stemmer\n",
    "\n",
    "from arango import ArangoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification of the document paths\n",
    "\n",
    "- download from: [Kaggle](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)\n",
    "- extract folder content to `{ProjectDir}/dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = join(pardir, 'dataset')\n",
    "\n",
    "json_paths = [\n",
    "    join(root_dir, 'arxiv', 'arxiv', 'pdf_json'),\n",
    "    join(root_dir, 'arxiv', 'arxiv', 'pdf_json'),\n",
    "    join(root_dir, 'comm_use_subset', 'comm_use_subset', 'pdf_json'),\n",
    "    join(root_dir, 'noncomm_use_subset', 'noncomm_use_subset', 'pdf_json'),\n",
    "    join(root_dir, 'custom_license', 'custom_license', 'pdf_json'),\n",
    "    join(root_dir, 'biorxiv_medrxiv', 'biorxiv_medrxiv', 'pdf_json'),\n",
    "]\n",
    "\n",
    "files = []\n",
    "[files.extend(glob(join(path, '*.json'))) for path in json_paths];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the document index\n",
    "\n",
    "Each document gets stored in the document_index collection with the following layout: \n",
    "```json\n",
    "{\n",
    "    _id: ..., \n",
    "    document_title: ...\n",
    "}\n",
    "```\n",
    "\n",
    "## Building the inverted index\n",
    "\n",
    "```json\n",
    "{\n",
    "    'stem': ...,\n",
    "    'doc_ids': [\n",
    "        {\n",
    "            {'doc_id': ..., \n",
    "             'count': {\n",
    "                 'title': ...,\n",
    "                 'abstract': ...,\n",
    "                 'body_text': ...}\n",
    "             },\n",
    "             {'doc_id': ..., \n",
    "             'count': {\n",
    "                 'title': ...,\n",
    "                 'abstract': ...,\n",
    "                 'body_text': ...}\n",
    "             }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = NLPPipeline([\n",
    "    ToLowercase(),\n",
    "    CitationRemover(),\n",
    "    ContentInBracketsRemover(),\n",
    "    Tokenizer(),\n",
    "    NonAlphanumericRemover(),\n",
    "    #SymbolRemover(),\n",
    "    StopwordRemover(),\n",
    "    Stemmer()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_database():\n",
    "    client = ArangoClient(hosts='http://localhost:8529')\n",
    "    sys_db = client.db('_system', username='root', password='')\n",
    "\n",
    "    if 'covid_19' not in sys_db.databases():\n",
    "        sys_db.create_database('covid_19')\n",
    "\n",
    "    db = client.db('covid_19', username='root', password='')\n",
    "\n",
    "    if not db.has_collection('inverted_index'):\n",
    "        db.create_collection('inverted_index')\n",
    "    if not db.has_collection('document_index'):\n",
    "        db.create_collection('document_index')\n",
    "\n",
    "    return db, db.collection('document_index'), db.collection('inverted_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connection_instance():\n",
    "    client = ArangoClient(hosts='http://localhost:8529')\n",
    "    db = client.db('covid_19', username='root', password='')\n",
    "    return db.collection('document_index'), db.collection('inverted_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db, document_index_collection, inverted_index_collection = initialize_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_object = {\n",
    "    'title': {'title': 1, 'abstract': 0, 'body_text': 0},\n",
    "    'abstract': {'title': 0, 'abstract': 1, 'body_text': 0},\n",
    "    'body_text': {'title': 0, 'abstract': 0, 'body_text': 1}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_get_doc_id(json_object, doc_id):\n",
    "    # TODO: O(n) --> improve with b-tree ? \n",
    "    res = list(filter(lambda f: (f[\"doc_id\"] == doc_id), json_object['doc_ids']))\n",
    "    if len(res) == 0:\n",
    "        return None\n",
    "    return res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document(stem, doc_id, count_object):\n",
    "    return {'_key': stem, 'doc_ids': [{'doc_id': doc_id, 'count': count_object}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_document_index(doc_id, document_title, document_index_collection):\n",
    "    if str(doc_id) not in document_index_collection:\n",
    "        document_index_collection.insert({'_key': str(doc_id), 'document_title': document_title})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_inverted_index(doc_id, stemmed_tokens, section, inverted_index_collection):\n",
    "    #update inverted index\n",
    "    for stem in stemmed_tokens:\n",
    "        if stem is '' or stem is ' ': continue\n",
    "        if stem in inverted_index_collection:\n",
    "            #try to find current document id in stem entry\n",
    "            tmp = inverted_index_collection[stem]\n",
    "            doc_id_object = try_get_doc_id(tmp, doc_id)\n",
    "            if doc_id_object is not None:\n",
    "                # update occurrence of stem in document section\n",
    "                try_get_doc_id(tmp, doc_id)['count']['title'] += 1\n",
    "                inverted_index_collection.update(tmp)\n",
    "            else:\n",
    "                # add document id\n",
    "                tmp['doc_ids'].append({'doc_id': doc_id, 'count': count_object[section]})\n",
    "                inverted_index_collection.update(tmp)\n",
    "        else:\n",
    "            doc = create_document(stem, doc_id, count_object[section])\n",
    "            inverted_index_collection.insert(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "28.4 µs ± 227 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
    }
   ],
   "source": [
    "%%timeit\n",
    "get_connection_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(args, update_doc_idx=True, update_inv_indx=True):\n",
    "    # open a connection to the database\n",
    "    document_index_collection, inverted_index_collection = get_connection_instance()\n",
    "    #iterate over all documents in chunk\n",
    "    for fpath, doc_id in args:\n",
    "        doc_id = int(doc_id)\n",
    "        data_loader = DataLoader(fpath)\n",
    "\n",
    "        doc_title = get_document_title(fpath, data_loader)\n",
    "\n",
    "        #database should only contain english documents with an valid document title\n",
    "        if doc_title == '' or not is_english(doc_title):\n",
    "            continue\n",
    "\n",
    "        if doc_title not in document_index_collection:\n",
    "            update_document_index(\n",
    "                doc_id, \n",
    "                doc_title, \n",
    "                document_index_collection)\n",
    "\n",
    "        for section in get_sections():\n",
    "            text = get_section(fpath, section, dl=data_loader)\n",
    "            stemmed_tokens = pipeline.transform(text)\n",
    "            update_inverted_index(doc_id, stemmed_tokens, section, inverted_index_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(files, chunk_size=128):\n",
    "    doc_ids = list(range(len(files)))\n",
    "    chunks = list()\n",
    "    for i in range(0, len(files), chunk_size):\n",
    "        indices = np.array(doc_ids[i: min(i+chunk_size, len(files))])\n",
    "        chunks.append(list(zip(\n",
    "            files[i: min(i+chunk_size, len(files))], \n",
    "            doc_ids[i: min(i+chunk_size, len(files))])))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "DocumentParseError",
     "evalue": "bad collection name in document ID \"Perception of emergent epidemic of COVID-2019 / SARS CoV-2 on the Polish Internet\"",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDocumentParseError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-09bc19f7e9c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcreate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprocess_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-dbba3bfe7bed>\u001b[0m in \u001b[0;36mprocess_chunk\u001b[0;34m(args, update_doc_idx, update_inv_indx)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdoc_title\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument_index_collection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             update_document_index(\n\u001b[1;32m     17\u001b[0m                 \u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/arango/collection.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_rev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_status_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/arango/collection.py\u001b[0m in \u001b[0;36mhas\u001b[0;34m(self, document, rev, check_rev)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mraise\u001b[0m \u001b[0marango\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDocumentRevisionError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mrevisions\u001b[0m \u001b[0mmismatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \"\"\"\n\u001b[0;32m--> 547\u001b[0;31m         \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prep_from_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_rev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         request = Request(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/arango/collection.py\u001b[0m in \u001b[0;36m_prep_from_doc\u001b[0;34m(self, document, rev, check_rev)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0mdoc_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0mdoc_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/arango/collection.py\u001b[0m in \u001b[0;36m_validate_id\u001b[0;34m(self, doc_id)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdoc_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             raise DocumentParseError(\n\u001b[0;32m--> 113\u001b[0;31m                 'bad collection name in document ID \"{}\"'.format(doc_id))\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocumentParseError\u001b[0m: bad collection name in document ID \"Perception of emergent epidemic of COVID-2019 / SARS CoV-2 on the Polish Internet\""
     ]
    }
   ],
   "source": [
    "for chunk in create_chunks(files, chunk_size=1024):\n",
    "    process_chunk(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "source": [
    "pool = Pool()\n",
    "chunks = create_chunks(files)\n",
    "\n",
    "for _ in tqdm(pool.imap_unordered(process_chunk, chunks), total=len(chunks)):\n",
    "    pass\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbasecondaae23e32a32964a9396c6d021c6ebb9de"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}