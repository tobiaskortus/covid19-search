{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: pymongo in /home/tobias/anaconda3/lib/python3.7/site-packages (3.10.1)\nRequirement already satisfied: pycld2 in /home/tobias/anaconda3/lib/python3.7/site-packages (0.41)\nRequirement already satisfied: pandas in /home/tobias/anaconda3/lib/python3.7/site-packages (1.0.1)\nRequirement already satisfied: python-dateutil>=2.6.1 in /home/tobias/anaconda3/lib/python3.7/site-packages (from pandas) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /home/tobias/anaconda3/lib/python3.7/site-packages (from pandas) (2019.3)\nRequirement already satisfied: numpy>=1.13.3 in /home/tobias/anaconda3/lib/python3.7/site-packages (from pandas) (1.18.1)\nRequirement already satisfied: six>=1.5 in /home/tobias/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)\nRequirement already satisfied: nltk in /home/tobias/anaconda3/lib/python3.7/site-packages (3.4.5)\nRequirement already satisfied: six in /home/tobias/anaconda3/lib/python3.7/site-packages (from nltk) (1.14.0)\n"
    }
   ],
   "source": [
    "!pip install pymongo\n",
    "!pip install pycld2\n",
    "!pip install pandas\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import pycld2 as cld2\n",
    "\n",
    "from multiprocessing.pool import Pool\n",
    "from tqdm._tqdm_notebook import tqdm\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from os import getcwd, pardir\n",
    "from os.path import join, basename\n",
    "\n",
    "from utils.literature import DataLoader, get_document_title, is_english, get_section, get_sections, get_files, get_authors\n",
    "from utils.preprocessing import NLPPipeline, Tokenizer, Stemmer, ToLowercase, Lemmatizer, StopwordRemover, CitationRemover, SymbolRemover, ContentInBracketsRemover\n",
    "from glob import glob\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer #TODO: Try lemmatizer instead of stemmer\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient as DBClient\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification of the document paths\n",
    "\n",
    "- download from: [Kaggle](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)\n",
    "- extract folder content to `{ProjectDir}/dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = join(pardir, 'dataset')\n",
    "files = get_files(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the document index\n",
    "\n",
    "Each document gets stored in the document_index collection with the following layout: \n",
    "```json\n",
    "{\n",
    "    _id: ..., \n",
    "    document_title: ...,\n",
    "    authors: [\n",
    "        ...,\n",
    "        ...,\n",
    "        ...\n",
    "    ],\n",
    "    abstract: ...\n",
    "}\n",
    "```\n",
    "\n",
    "## Building the inverted index\n",
    "\n",
    "```json\n",
    "{\n",
    "    'stem': ...,\n",
    "    'doc_ids': [\n",
    "        {\n",
    "            {'doc_id': ..., \n",
    "             'count': {\n",
    "                 'title': ...,\n",
    "                 'abstract': ...,\n",
    "                 'body_text': ...}\n",
    "             },\n",
    "             {'doc_id': ..., \n",
    "             'count': {\n",
    "                 'title': ...,\n",
    "                 'abstract': ...,\n",
    "                 'body_text': ...}\n",
    "             }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with DBClient('localhost', 27017, w=0) as client:\n",
    "    db = client['covid_19']\n",
    "    inverted_index_collection = db['inverted_index']\n",
    "    inverted_index_collection.create_index([(\"doc_ids\", pymongo.ASCENDING)], background=True, name='doc_ids')\n",
    "    inverted_index_collection.create_index([(\"doc_ids.doc_id\", pymongo.ASCENDING)], background=True, name='doc_ids.doc_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = NLPPipeline([\n",
    "    ToLowercase(),\n",
    "    CitationRemover(),\n",
    "    ContentInBracketsRemover(),\n",
    "    Tokenizer(),\n",
    "    SymbolRemover(),\n",
    "    StopwordRemover(),\n",
    "    Stemmer()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_post(stem, doc_id, count_object):\n",
    "    return {'_id': stem, 'doc_ids': [{'doc_id': doc_id, 'count': count_object}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_object(section, count=1):\n",
    "    if section is 'title': return {'title': count, 'abstract': 0, 'body_text': 0}\n",
    "    elif section is 'abstract': return {'title': 0, 'abstract': count, 'body_text': 0}\n",
    "    elif section is 'body_text': return {'title': 0, 'abstract': 0, 'body_text': count}\n",
    "    else: raise Exception('Invalid section')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_document_index(doc_id, document_title, document_index_collection):\n",
    "    doc_id_entry = document_index_collection.find({'_id': doc_id}).limit(1)\n",
    "    if doc_id_entry.count() == 0:\n",
    "        document_index_collection.insert_one({'_id': doc_id, 'document_title': document_title})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_index_object(doc_id, document_title, authors, abstract):\n",
    "    return {'_id': doc_id, 'document_title': document_title, 'abstract': abstract, 'authors': authors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_inverted_index(doc_id, stemmed_tokens, section, inverted_index_collection):\n",
    "    #update inverted index\n",
    "    for stem, count in stemmed_tokens.items():\n",
    "        stem_entry = inverted_index_collection.find({'_id': stem}).limit(1)\n",
    "        if stem_entry.count() > 0:\n",
    "            #try to find current document id in stem entry\n",
    "            doc_id_object = inverted_index_collection.find(\n",
    "                {'_id': stem, \n",
    "                'doc_ids': {'$elemMatch': {'doc_id': doc_id}}}).limit(1)\n",
    "            if doc_id_object.count() > 0:\n",
    "                # update occurrence of stem in document section\n",
    "                inverted_index_collection.update(\n",
    "                    {'_id': stem, \n",
    "                    'doc_ids': {'$elemMatch': {'doc_id': doc_id}}},\n",
    "                    {'$inc': {f'doc_ids.$.count.{section}': count}})\n",
    "            else:\n",
    "                # add document id\n",
    "                inverted_index_collection.update(\n",
    "                    {'_id': stem},\n",
    "                    {'$push': {'doc_ids': {'doc_id': doc_id, 'count': create_count_object(section, count)}}})\n",
    "        else:\n",
    "            post = create_post(stem, doc_id, create_count_object(section, count))\n",
    "            inverted_index_collection.insert_one(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(args, update_doc_idx=True, update_inv_indx=True):\n",
    "    with DBClient('localhost', 27017, w=0) as client:\n",
    "\n",
    "        #open a connection to the database\n",
    "        db = client['covid_19']\n",
    "        document_index_collection = db['document_index']\n",
    "        inverted_index_collection = db['inverted_index']\n",
    "\n",
    "        document_objects = []\n",
    "        \n",
    "        #iterate over all documents in chunk\n",
    "        for fpath, doc_id in args:\n",
    "            doc_id = int(doc_id)\n",
    "            data_loader = DataLoader(fpath)\n",
    "\n",
    "            doc_title = get_document_title(fpath, data_loader)\n",
    "\n",
    "            if doc_title == '' or not is_english(doc_title):\n",
    "                continue\n",
    "\n",
    "            #database should only contain english documents with an valid document title\n",
    "            if update_doc_idx:\n",
    "                abstract = get_section(fpath,'abstract', data_loader)[:500] + \"...\"\n",
    "                authors = get_authors(fpath, data_loader)\n",
    "                document_index_collection.insert_one(create_document_index_object(doc_id, doc_title, authors, abstract))\n",
    "                #update_document_index(\n",
    "                #    doc_id, \n",
    "                #    doc_title, \n",
    "                #    document_index_collection)\n",
    "\n",
    "            if not update_inverted_index:\n",
    "                continue\n",
    "            \n",
    "            for section in get_sections():\n",
    "                text = get_section(fpath, section, dl=data_loader)\n",
    "                stemmed_tokens = pipeline.transform(text)\n",
    "                stemmed_tokens_reduced = Counter(stemmed_tokens)\n",
    "                update_inverted_index(doc_id, stemmed_tokens_reduced, section, inverted_index_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(files, chunk_size=128):\n",
    "    doc_ids = list(range(len(files)))\n",
    "    chunks = list()\n",
    "    for i in range(0, len(files), chunk_size):\n",
    "        indices = np.array(doc_ids[i: min(i+chunk_size, len(files))])\n",
    "        chunks.append(list(zip(\n",
    "            files[i: min(i+chunk_size, len(files))], \n",
    "            doc_ids[i: min(i+chunk_size, len(files))])))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=385.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "725d8f5048704209957c55bea4085a36"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    }
   ],
   "source": [
    "pool = Pool()\n",
    "chunks = create_chunks(files)\n",
    "\n",
    "for _ in tqdm(pool.imap_unordered(process_chunk, chunks), total=len(chunks)):\n",
    "    pass\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance monitoring\n",
    "\n",
    "~ 20 0000 documents in database\n",
    "\n",
    "Backlog: automatische Übersetzung\n",
    "Neo4j semantic web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "%%timeit -n 100\n",
    "with DBClient('localhost', 27017, w=0) as client:\n",
    "    #open a connection to the database\n",
    "    db = client['covid_19']\n",
    "    document_index_collection = db['document_index']\n",
    "    inverted_index_collection = db['inverted_index']\n",
    "```\n",
    "\n",
    "785 µs ± 46.7 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "%%timeit -n 100\n",
    "data_loader = DataLoader(files[0])\n",
    "doc_title = get_document_title(files[0], data_loader)\n",
    "```\n",
    "\n",
    "496 µs ± 27.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "data_loader = DataLoader(files[0])\n",
    "abstract = get_section(files[0], 'abstract', data_loader)\n",
    "\n",
    "%%timeit -n 100\n",
    "is_english(abstract)\n",
    "```\n",
    "\n",
    "48.8 µs ± 1.02 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "%%timeit -n 100\n",
    "pipeline.transform(doc_title)\n",
    "```\n",
    "\n",
    "1.35 ms ± 43.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "%%timeit\n",
    "inverted_index_collection.find({'_id': 'detect'}).limit(1).count()\n",
    "```\n",
    "\n",
    "1.35 ms ± 43.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "%%timeit\n",
    "doc_id_object = inverted_index_collection.find(\n",
    "                {'_id': 'detect', \n",
    "                'doc_ids': {'$elemMatch': {'doc_id': 312}}}).limit(1).count()\n",
    "```\n",
    "\n",
    "1.35 ms ± 43.6 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "import uuid\n",
    "\n",
    "%%timeit\n",
    "post = create_post(uuid.uuid4(), -1, count_object['abstract'])\n",
    "inverted_index_collection.insert_one(post)\n",
    "```\n",
    "\n",
    "98.7 µs ± 122 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "%%timeit\n",
    "inverted_index_collection.update(\n",
    "        {'_id': 'testtoken', \n",
    "        'doc_ids': {'$elemMatch': {'doc_id': -1}}},\n",
    "        {'$inc': {f'doc_ids.$.count.abstract': 1}})\n",
    "```\n",
    "\n",
    "99.2 µs ± 999 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "%%timeit -n 1000\n",
    "inverted_index_collection.update(\n",
    "                    {'_id': 'testtoken'},\n",
    "                    {'$push': {'doc_ids': {'doc_id': -2, 'count': count_object['abstract']}}})\n",
    "```\n",
    "\n",
    "99.2 µs ± 999 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbaseconda685979d1612e4539ab4b182671d44102"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}