{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import pardir\n",
    "from os.path import join\n",
    "from utils.literature import get_files, get_document_title, get_authors, get_bib_entries, DataLoader, is_english\n",
    "from utils.processing import create_chunks\n",
    "from utils.metadata import CORDMetadata\n",
    "from utils.grid.grid import GridLookup\n",
    "from neo4j import GraphDatabase\n",
    "from multiprocessing.pool import Pool\n",
    "from tqdm._tqdm_notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = ('neo4j', 'password')\n",
    "address = f'bolt://localhost:7687'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Split Query in smaller chunks\n",
    "def add_new(tx, title, doc_id, author, institution, journal):\n",
    "    tx.run(\"MERGE (a:Document {title: $title, doc_id: $doc_id})\"\n",
    "           \"MERGE (b:Author {name: $author})\"\n",
    "           \"MERGE (c:Institution {name: $institution_name})\"\n",
    "           \"MERGE (d:Journal {name: $journal})\"\n",
    "           \"MERGE (e:Country {name: $country, code: $code})\"\n",
    "           \"MERGE (b)-[:WORKS_FOR]->(c)\"\n",
    "           \"MERGE (c)-[:EMPLOYED]->(b)\"\n",
    "           \"MERGE (a)-[:WRITTEN_BY]->(b)\"\n",
    "           \"MERGE (b)-[:WROTE]->(a)\"\n",
    "           \"MERGE (c)-[:LOCATED_IN]->(e)\"\n",
    "           \"MERGE (e)-[:LOCATES]->(c)\"\n",
    "           \"MERGE (a)-[:PUBLISHED_IN]->(d)\"\n",
    "           \"MERGE (d)-[:PUBLISHED]->(a)\",\n",
    "           title=title, doc_id=doc_id, author=author, institution_name=institution['Name'], \n",
    "           country = institution['Country'], code=institution['Code'], journal=journal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bib_query(tx, title, ref_title, author):\n",
    "    tx.run(\"MATCH (a:Document {title: $title}) \"\n",
    "           \"MERGE (b:Document {title: $ref_title, doc_id: $doc_id}) \"\n",
    "           \"MERGE (c:Author {name: $author}) \"\n",
    "           \"MERGE (a)-[:REFERENCED]->(b) \"\n",
    "           \"MERGE (b)-[:REFERENCED_BY]->(a) \"\n",
    "           \"MERGE (b)-[:WRITTEN_BY]->(c) \"\n",
    "           \"MERGE (c)-[:WROTE]->(b)\", \n",
    "            title=title, ref_title=ref_title, author=author, doc_id=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_bib_query(tx, title, ref_title):\n",
    "    tx.run(\"\"\n",
    "           \"MERGE (b:Document {title: $ref_title}) \"\n",
    "           \"MERGE (a)-[:REFERENCED]->(b) \"\n",
    "           \"MERGE (b)-[:REFERENCED_BY]->(a)\", title=title, ref_title=ref_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_author_query(tx, title, author):\n",
    "    tx.run(\"MATCH (a:Document {title: $title}) \"\n",
    "           \"MERGE (b:Author {name: $author}) \"\n",
    "           \"MERGE (a)-[:WRITTEN_BY]->(b) \"\n",
    "           \"MERGE (b)-[:WROTE]->(a)\", \n",
    "           title=title, author=author)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = join(pardir, 'dataset')\n",
    "files = get_files(root_dir)\n",
    "metadata_lookup = CORDMetadata()\n",
    "grid_lookup = GridLookup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "undefined = {\n",
    "    'Name': 'undefined', \n",
    "    'Country': 'undefined', \n",
    "    'Code': 'undefined'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk_documents(args):\n",
    "    with GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"), encrypted=False) as driver:  \n",
    "        with driver.session() as session:      \n",
    "            #iterate over all documents in chunk\n",
    "            for fpath, doc_id in args:\n",
    "                doc_id = int(doc_id)\n",
    "                data_loader = DataLoader(fpath, grid_lookup)\n",
    "\n",
    "                doc_title = get_document_title(fpath, data_loader)\n",
    "                \n",
    "                sha = data_loader.get_paper_id()\n",
    "                journal = metadata_lookup.get_journal(sha)\n",
    "\n",
    "                #database should only contain english documents with an valid document title\n",
    "                if doc_title == '' or not is_english(doc_title):\n",
    "                    continue\n",
    "\n",
    "                #avoid duplicates from dataset\n",
    "                doc_exists = added.get(doc_title) is not None\n",
    "                if doc_exists:\n",
    "                    continue\n",
    "                \n",
    "                added[doc_title] = True\n",
    "            \n",
    "                # enable normalize names in order to match ref entries with existing documents\n",
    "                authors = get_authors(fpath, data_loader, plausibility_check=True, clean_names=True, normalize_names=False) \n",
    "                \n",
    "                for author, institution in authors:\n",
    "                    if author is None: continue\n",
    "                    institution = institution if institution is not None else undefined\n",
    "                    journal = journal if journal is not None and journal != 'nan' else 'undefined'\n",
    "                    session.write_transaction(add_new, doc_title, doc_id, author, institution, journal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk_bib_ref(args):\n",
    "    with GraphDatabase.driver(\"bolt://localhost:7687\", auth=(\"neo4j\", \"password\"), encrypted=False) as driver:  \n",
    "        with driver.session() as session:      \n",
    "            #iterate over all documents in chunk\n",
    "            for fpath, doc_id in args:\n",
    "                doc_id = int(doc_id)\n",
    "\n",
    "                data_loader = DataLoader(fpath, grid_lookup)\n",
    "                doc_title = get_document_title(fpath, data_loader)\n",
    "\n",
    "                #database should only contain english documents with an valid document title\n",
    "                if doc_title == '' or not is_english(doc_title):\n",
    "                    continue\n",
    "                \n",
    "                ref_entries = get_bib_entries(fpath, data_loader)\n",
    "\n",
    "                for bib_title, authors in ref_entries:\n",
    "                    #try to merge ref entry with existing document\n",
    "                    doc_exists = added.get(doc_title) is not None\n",
    "\n",
    "                    if not doc_exists:\n",
    "                        for author, _ in authors:\n",
    "                            if author is None: continue\n",
    "                            session.write_transaction(add_bib_query, doc_title, bib_title, author)\n",
    "                    else:\n",
    "                        session.write_transaction(mark_bib_query, doc_title, bib_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=385.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7dc28777b6d345f7949450220a698653"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n"
    }
   ],
   "source": [
    "# ache of added documents, used in process_chunk_bib_ref \n",
    "# for checking if ref entry is already existent in the database \n",
    "# --> improve performance\n",
    "added = dict()\n",
    "\n",
    "pool = Pool() #FIXME: Each thread requires large ammount of ram \n",
    "chunks = create_chunks(files)\n",
    "\n",
    "for _ in tqdm(pool.imap_unordered(process_chunk_documents, chunks), total=len(chunks)):\n",
    "    pass\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'\\nCurrently not performant enough to run in acceptable time\\n'"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "\"\"\"\n",
    "Currently not performant enough to run in acceptable time\n",
    "\"\"\"\n",
    "\n",
    "#pool = Pool() #FIXME: Each thread requires large ammount of ram \n",
    "#chunks = create_chunks(files)\n",
    "#\n",
    "#for _ in tqdm(pool.imap_unordered(process_chunk_bib_ref, chunks), total=len(chunks)):\n",
    "#    pass\n",
    "\n",
    "#pool.close()\n",
    "#pool.join()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}