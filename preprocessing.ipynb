{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: pymongo in /home/tobias/anaconda3/lib/python3.7/site-packages (3.10.1)\n"
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import string\n",
    "import re\n",
    "import sys\n",
    "\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from os import getcwd\n",
    "from os.path import join, basename\n",
    "\n",
    "from utils.literature import DataLoader\n",
    "from glob import glob\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer #TODO: Try lemmatizer instead of stemmer\n",
    "\n",
    "import pymongo\n",
    "from pymongo import MongoClient as DBClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specification of the document paths\n",
    "\n",
    "- download from: [Kaggle](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)\n",
    "- extract folder content to `{ProjectDir}/dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = getcwd()\n",
    "\n",
    "json_paths = [\n",
    "    join(cwd, 'dataset', 'arxiv', 'arxiv', 'pdf_json'),\n",
    "    join(cwd, 'dataset', 'arxiv', 'arxiv', 'pdf_json'),\n",
    "    join(cwd, 'dataset', 'comm_use_subset', 'comm_use_subset', 'pdf_json'),\n",
    "    join(cwd, 'dataset', 'noncomm_use_subset', 'noncomm_use_subset', 'pdf_json'),\n",
    "    join(cwd, 'dataset', 'custom_license', 'custom_license', 'pdf_json'),\n",
    "    join(cwd, 'dataset' 'biorxiv_medrxiv', 'biorxiv_medrxiv', 'pdf_json'),\n",
    "]\n",
    "string.punctuation\n",
    "files = []\n",
    "[files.extend(glob(join(path, '*.json'))) for path in json_paths];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and updating the metadata information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(join(getcwd(), 'dataset', 'metadata.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata['doc_id'] = np.arange(len(metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = DBClient('localhost', 27017, w=0)\n",
    "db = client['covid_19']\n",
    "document_index_collection = db['document_index']\n",
    "reversed_index_collection = db['reversed_index']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the document index\n",
    "\n",
    "Each document gets stored in the document_index collection with the following layout: \n",
    "```json\n",
    "{\n",
    "    _id: ..., \n",
    "    document_title: ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_idxs = dict(zip(metadata['doc_id'], metadata['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _id, document_title in doc_idxs.items():\n",
    "    doc_id = document_index_collection.find({'_id': _id}).limit(1)\n",
    "    if doc_id.count() == 0:\n",
    "        document_index_collection.insert_one({'_id': _id, 'document_title': document_title})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the reversed index\n",
    "\n",
    "```json\n",
    "{\n",
    "    'stem': ...,\n",
    "    'doc_ids': [\n",
    "        {\n",
    "            {'doc_id': ..., 'count': ...},\n",
    "            {'doc_id': ..., 'count': ...},\n",
    "            {'doc_id': ..., 'count': ...},\n",
    "            {'doc_id': ..., 'count': ...},\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Test\n",
    "pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "tokenizer = RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(metadata, fpath):\n",
    "    sha = basename(fpath)[:-5]\n",
    "    return sha, metadata.loc[metadata['sha'] == sha]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_text(fpath):\n",
    "    dl = DataLoader(fpath)\n",
    "    return dl.get_full_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_post(stem, doc_id):\n",
    "    return {'_id': stem, 'doc_ids': [{'doc_id': doc_id, 'count': 1}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def process_file(args):\n",
    "    fpath, doc_id = args\n",
    "    doc_id = int(doc_id)\n",
    "    #preprocessing\n",
    "    text = get_full_text(fpath)\n",
    "    text = text.strip()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    filtered_tokens = [w for w in tokens if not w in stop_words] \n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "\n",
    "    for stem in stemmed_tokens:\n",
    "        stem_entry = reversed_index_collection.find({'_id': stem}).limit(1)\n",
    "        if stem_entry.count() > 0:\n",
    "            #try to find current document id in stem entry\n",
    "            doc_id_object = reversed_index_collection.find(\n",
    "                {'_id': stem, \n",
    "                 'doc_ids': {'$elemMatch': {'doc_id': doc_id}}}).limit(1)\n",
    "            if doc_id_object.count() > 0:\n",
    "                # update occurrence of stem in document\n",
    "                reversed_index_collection.update(\n",
    "                    {'_id': stem, \n",
    "                     'doc_ids': {'$elemMatch': {'doc_id': doc_id}}},\n",
    "                    {'$inc': {'doc_ids.$.count': 1}})\n",
    "            else:\n",
    "                 # add document id\n",
    "                 reversed_index_collection.update(\n",
    "                    {'_id': stem},\n",
    "                    {'$push': {'doc_ids': {'doc_id': doc_id, 'count': 1}}})\n",
    "        else:\n",
    "            post = create_post(stem, doc_id)\n",
    "            reversed_index_collection.insert_one(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "\n\n\n\n\n\n\n  0%|          | 0/46528 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
    },
    {
     "output_type": "error",
     "ename": "InvalidDocument",
     "evalue": "cannot encode object: 7, of type: <class 'numpy.int64'>",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/tobias/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"<ipython-input-26-5fbd1b23f847>\", line 17, in process_file\n    if doc_id_object.count() > 0:\n  File \"/home/tobias/anaconda3/lib/python3.7/site-packages/pymongo/cursor.py\", line 787, in count\n    cmd, self.__collation, session=self.__session)\n  File \"/home/tobias/anaconda3/lib/python3.7/site-packages/pymongo/collection.py\", line 1600, in _count\n    _cmd, self._read_preference_for(session), session)\n  File \"/home/tobias/anaconda3/lib/python3.7/site-packages/pymongo/mongo_client.py\", line 1464, in _retryable_read\n    return func(session, server, sock_info, slave_ok)\n  File \"/home/tobias/anaconda3/lib/python3.7/site-packages/pymongo/collection.py\", line 1594, in _cmd\n    session=session)\n  File \"/home/tobias/anaconda3/lib/python3.7/site-packages/pymongo/collection.py\", line 250, in _command\n    user_fields=user_fields)\n  File \"/home/tobias/anaconda3/lib/python3.7/site-packages/pymongo/pool.py\", line 618, in command\n    self._raise_connection_failure(error)\n  File \"/home/tobias/anaconda3/lib/python3.7/site-packages/pymongo/pool.py\", line 613, in command\n    user_fields=user_fields)\n  File \"/home/tobias/anaconda3/lib/python3.7/site-packages/pymongo/network.py\", line 129, in command\n    codec_options, ctx=compression_ctx)\n  File \"/home/tobias/anaconda3/lib/python3.7/site-packages/pymongo/message.py\", line 707, in _op_msg\n    flags, command, identifier, docs, check_keys, opts)\nbson.errors.InvalidDocument: cannot encode object: 7, of type: <class 'numpy.int64'>\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mInvalidDocument\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-46aab2de1693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1105\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidDocument\u001b[0m: cannot encode object: 7, of type: <class 'numpy.int64'>"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import tqdm\n",
    "pool = Pool(processes=8)\n",
    "args = list(zip(files, np.arange(len(files))))\n",
    "\n",
    "for _ in tqdm.tqdm(pool.imap_unordered(process_file, args), total=len(files)):\n",
    "    pass\n",
    "\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "('/home/tobias/Desktop/covid19-search/dataset/arxiv/arxiv/pdf_json/f5a816cbca04dc4caa8f0f73e37ac7387d67b402.json', 0)\n"
    }
   ],
   "source": [
    "print(args[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python37664bitbasecondaae23e32a32964a9396c6d021c6ebb9de"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}